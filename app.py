#!/user/bin/env python3
"""
This tool built to keep track of CVE list provided by NVD, and display the list in a table format
the tool creates a new thread every (x) seconds to request last modified meta date
and check if there's an update since last request.
"""
__author__ = "Nourah Altawallah"
__copyright__ = "Copyright 2020"
__license__ = "GPL"
__version__ = "1.0.10"
__date__ = "2020-01-06"
__maintainer__ = "Fahad Alduraibi"
__email__ = "fahad@fadvisor.net"

import datetime
import json
import random
import time
import flashtext   
import gzip
import threading
from flask import Flask, render_template, jsonify, Response,request, redirect
import requests
from io import BytesIO
import logging
import os
import configparser
import ast


def read_configurations():
	config = configparser.ConfigParser()
	current_path = os.path.dirname(__file__)
	filename = os.path.join(current_path, 'config/config.ini')
	ret = config.read(filename)
	if not ret:
		logging.error("Cannot find the configurations file!, kindly check the config folder and create a new config.ini based on the provided sample.")
		quit()
	
	global PROXY
	if config.has_option('CONNECTIONS','Proxy'):
		PROXY = config.get('CONNECTIONS','Proxy')
		if PROXY:
			PROXY = ast.literal_eval(PROXY)
	else:
		PROXY = ''

	global SSL_VERIFY
	SSL_VERIFY = ast.literal_eval(config.get('CONNECTIONS','SSL_Verify'))
	
	global CVE_META_URL
	CVE_META_URL = config.get('SOURCES','CVE_META_URL')
	
	global CVE_JSON_URL
	CVE_JSON_URL = config.get('SOURCES','CVE_JSON_URL')
	
	global META_Check_Interval
	META_Check_Interval = ast.literal_eval(config.get('DEFAULT','META_Check_Interval'))
	
	global Re_Check_Interval
	Re_Check_Interval = ast.literal_eval(config.get('DEFAULT','Re_Check_Interval'))

read_configurations()




def datetime_converter(content):
	# convert datetime data type to string
	if isinstance(content, datetime.datetime):
		return content.__str__()


def data_sender():
	while True: 
		# calculate the duration in second before the next request
		now = datetime.datetime.now()
		sec_left = (now - MyVariables.last_req).seconds
		next_req = abs(MyVariables.interval - sec_left)
		# send content to "list" event listener 
		json_content = json.dumps(MyVariables.content,default = datetime_converter)
		yield f"event:list\n"
		yield f"data:{json_content}\n\n" 
		
		# send msg to "msg" event listener 
		json_msg = json.dumps(MyVariables.msg) 
		yield f"event: msg\n"
		yield f"data:{json_msg}\n\n" 

		time.sleep(next_req +10)


def check_meta_date(meta_date, content, meta_file):
	meta_file = meta_file.decode("utf-8")
	modified_date = meta_file.split('\n', 1)[0].split(':', 1)[1].split(':', 1)[0]
	modified_date = datetime.datetime.strptime(modified_date, '%Y-%m-%dT%H')

	if modified_date > meta_date:
		meta_date = modified_date
		status = 'fine'

		try:
			# Request the latest CVE list from nvd.nist.gov
			buffer=requests.get(CVE_JSON_URL ,proxies=PROXY, verify=SSL_VERIFY)
			
			# Decompress the downloaded file 
			json_bytes = gzip.GzipFile(fileobj=BytesIO(buffer.content)).read()
			status, msg, content = get_content(status, content, json_bytes)
 
		except Exception as err:
			print("ERROR: " + str(err))
			status = 'error'
			msg = "Downloading of the CVE file has failed!"
			logging.error("Error downloading the CVE file:", exc_info=True)

	# No new data
	else:
		status = 'fine'
		content, status, msg = re_filter_content(content, status)
 
	return meta_date, status, msg, content


def add_keywords(keyword_processor):
	# add and highlight keywords 
	current_path = os.path.dirname(__file__)
	filename = os.path.join(current_path, 'config/keywords.txt')
	try:
		keyword_processor.add_keyword_from_file(filename)
	except Exception as err:
		logging.error(f'Cannot find {filename} file!', exc_info=True)


def get_content(status, content, json_bytes):
	cve_list = []
	# get UTC current time
	date = datetime.datetime.utcnow()
	json_str = json_bytes.decode('utf-8')
	cve_data = json.loads(json_str)
	extracted_list = {}
	stored_list = cve_data['CVE_Items']

	keyword_processor = flashtext.KeywordProcessor()
	add_keywords(keyword_processor)

	# collect cve info where is last update date < 24 hour
	for index in range(len(stored_list)):
		last_mod_object = datetime.datetime.strptime(stored_list[index]['lastModifiedDate'],'%Y-%m-%dT%H:%MZ')
		publish_object = datetime.datetime.strptime(stored_list[index]['publishedDate'],'%Y-%m-%dT%H:%MZ')
	# Only list records that has the CVSS-V3 scores (old records have only V2 and we don't need them)
		if 'baseMetricV3' in stored_list[index]['impact']:
			if date - last_mod_object <= datetime.timedelta(hours=24) or date - publish_object <= datetime.timedelta(hours=24):
                                # highlight present keywords and  get priority
				desc , prio = keyword_processor.highlight_keywords(stored_list[index]['cve']['description']['description_data'][0]['value']);
				extracted_list = {'URL':('https://nvd.nist.gov/vuln/detail/'+stored_list[index]['cve']['CVE_data_meta']['ID']),
					'ID': stored_list[index]['cve']['CVE_data_meta']['ID'],
					'Description': desc,
					'priority' :  prio,
					'Score': stored_list[index]['impact']['baseMetricV3']['cvssV3']['baseScore'],
					'Severity': stored_list[index]['impact']['baseMetricV3']['cvssV3']['baseSeverity'],
					'Last_Updated': (
						datetime.datetime.strptime(str(last_mod_object + datetime.timedelta(seconds=3600 * 3)), '%Y-%m-%d %H:%M:%S')),
				}
				cve_list.append(extracted_list)

	if len(cve_list) > 0: # sort new content
		content, status, msg = sort_content(cve_list, content, status)

	else: # no new content ->filter current content
		content, status, msg = re_filter_content(content, status)

	return status, msg, content


def sort_content(cve_list, content, status):
	# Sort by recent updated cve ,if two IDs have the same hours then put the highest score first
	if len(cve_list) > 0:
		sort = sorted(cve_list, key=lambda x: (x['priority'], x['Score'], x['Last_Updated']), reverse=True)
		content = sort
		msg = ''
	else:
		msg = 'No new data since last 24 hours'

	return content, status, msg


def re_filter_content(content, status):
	"""
	 re-filter current content to remove any old data(24h since it last modified)
	"""
	# get local date&time
	date = datetime.datetime.now()
	list2 = []

	if len(content) > 0:
		for dic in content:
			for key in dic:
				if key == 'Last_Updated':
					if date - dic[key] < datetime.timedelta(days=1, hours=24):
						list2.append(dic)

	content, status, msg = sort_content(list2, content, status)
 
	return content, status, msg


def main():
	try:
		buffer = requests.get(CVE_META_URL, proxies=PROXY, verify=SSL_VERIFY)
		buffer.raise_for_status()
		meta_file = buffer.content
		MyVariables.interval = META_Check_Interval
		threading.Timer(MyVariables.interval, main).start()
		MyVariables.last_req = datetime.datetime.now()

		MyVariables.meta_date, MyVariables.status, MyVariables.msg, MyVariables.content = check_meta_date(MyVariables.meta_date, MyVariables.content, meta_file)

	# invalid HTTP response
	except requests.exceptions.HTTPError as err:
		MyVariables.status = 'error'
		logging.error(err)

		# check connection after x seconds
		MyVariables.interval = Re_Check_Interval
		threading.Timer(MyVariables.interval, main).start()
		MyVariables.last_req = datetime.datetime.now()
		if "Error 404" in MyVariables.msg:
			MyVariables.msg = "Nist Page Not Found"
		else:
			MyVariables.msg = "Http Error:" +str(err)
	
	 # network problems
	except requests.exceptions.ConnectionError as err:
		MyVariables.status = 'error'
		MyVariables.msg = "Internet Connection Problem"
		logging.error("Internet Connection Problem", exc_info=True)

		# check connection after x seconds
		MyVariables.interval = Re_Check_Interval
		threading.Timer(MyVariables.interval, main).start()
		MyVariables.last_req = datetime.datetime.now()
	
	# request exceeds the configured number of maximum redirection
	except requests.exceptions.Timeout as err:
		MyVariables.status = 'error'
		MyVariables.msg = "Request Timeout Error"
		logging.error("Request Timeout Error", exc_info=True)

		# check connection after x seconds
		MyVariables.interval = Re_Check_Interval
		threading.Timer(MyVariables.interval, main).start()
		MyVariables.last_req = datetime.datetime.now()


	# All other exceptions
	except requests.exceptions.RequestException as err:
		MyVariables.status = 'error'
		MyVariables.msg = "Error:"+str(err)
		logging.error(err)

		# check connection after x seconds
		MyVariables.interval = Re_Check_Interval
		threading.Timer(MyVariables.interval, main).start()
		MyVariables.last_req = datetime.datetime.now()
 
class MyVariables:
	"""
	this class created to initiate and keep meta_date, content , status, msg,interval,last_req current values
	"""
	meta_date = datetime.datetime.strptime('2019-07-04T01', '%Y-%m-%dT%H')
	content = ''
	status = 'error'
	msg = ''
	interval = Re_Check_Interval
	last_req = datetime.datetime.now()
 

main()

# app section
app = Flask(__name__)

# return view.html to each client first request
@app.route('/')
def view_page():
	return render_template('view.html', version=__version__)
 
@app.route('/data', methods=['GET'])
def data():
	return Response(data_sender(), mimetype='text/event-stream')


if __name__ == '__main__':
	app.run(threaded = True)
	#app.run(debug = True)
